{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wiki_scrap(url_add):\n",
    "    url= url_add\n",
    "    html = urlopen(url)\n",
    "    bs = BeautifulSoup(html, \"html.parser\")\n",
    "    titles = bs.find_all(['h1', 'h2','h3','h4','h5','h6'])\n",
    "    print('List all the header tags :', *titles, sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.wikipedia.org\"\n",
    "wiki_scrap(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://www.imdb.com/list/ls091520106/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imdb(url_add):\n",
    "    url = url_add\n",
    "    r=requests.get(url)\n",
    "    soup=BeautifulSoup(r.content)\n",
    "    name=[]\n",
    "    rating=[]\n",
    "    year_of_release=[]\n",
    "    detail=soup.find('div',class_='ipl-rating-star small')\n",
    "    details_movie=soup.find_all('h3',class_='lister-item-header')\n",
    "    for details in details_movie:\n",
    "        name.append(details.text.split(\"\\n\")[2])\n",
    "        year_of_release.append(details.text.split(\"\\n\")[3].replace(\"(\",\"\").replace(\")\",\"\"))\n",
    "\n",
    "    details_rating=soup.find_all('div',class_='ipl-rating-star small')\n",
    "    for details in details_rating:\n",
    "        rating.append(details.text.split(\"\\n\")[8])\n",
    "\n",
    "    lst=list(zip(name,year_of_release,rating))\n",
    "    df=pd.DataFrame(lst,columns=['Title','Years','Rating'])\n",
    "    print(df.set_index('Title'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://www.imdb.com/list/ls009997493/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def desiIMDB(url_add):\n",
    "    r=requests.get(url)\n",
    "    soup=BeautifulSoup(r.content)\n",
    "    name=[]\n",
    "    rating=[]\n",
    "    year_of_release=[]\n",
    "    detail=soup.find('div',class_='ipl-rating-star small')\n",
    "    details_movie=soup.find_all('h3',class_='lister-item-header')\n",
    "    for details in details_movie:\n",
    "        name.append(details.text.split(\"\\n\")[2])\n",
    "        year_of_release.append(details.text.split(\"\\n\")[3].replace(\"(\",\"\").replace(\")\",\"\"))\n",
    "    details_rating=soup.find_all('div',class_='ipl-rating-star small')\n",
    "    for details in details_rating:\n",
    "        rating.append(details.text.split(\"\\n\")[8])\n",
    "    lst=list(zip(name,year_of_release,rating))\n",
    "    df=pd.DataFrame(lst,columns=['Name','Year of Release','Rating'])\n",
    "    print(df.set_index('Name'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desiIMDB(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def president(url):\n",
    "    president_name=[]\n",
    "    term=[]\n",
    "    page= requests.get(url)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    for i in soup.find_all('div', class_=\"presidentListing\"):    \n",
    "        president_name.append(i.text.split('\\n')[1].split('(')[0])\n",
    "        term.append(i.text.split('\\n')[2].split(':')[1])\n",
    "    df=pd.DataFrame({\"President\":president_name, 'Term': term})\n",
    "    print(df.set_index('President'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url ='https://presidentofindia.nic.in/former-presidents.htm'\n",
    "president(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://www.icc-cricket.com/rankings/mens/team-rankings/odi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def icc_men(url):\n",
    "    r=requests.get(url)\n",
    "    soup=BeautifulSoup(r.content)\n",
    "\n",
    "    rankings=[]\n",
    "    teams=[]\n",
    "    matches=[]\n",
    "    points=[]\n",
    "    ratings=[]\n",
    "\n",
    "\n",
    "    team1=soup.find('tr',class_='rankings-block__banner')\n",
    "    rankings.append(team1.text.split(\"\\n\")[1])\n",
    "    teams.append(team1.text.split(\"\\n\")[4])\n",
    "    matches.append(team1.text.split(\"\\n\")[7])\n",
    "    points.append(team1.text.split(\"\\n\")[8])\n",
    "    ratings.append(team1.text.split(\"\\n\")[10].split(\" \")[28])\n",
    "\n",
    "    other_teams=soup.find_all('tr',class_='table-body')\n",
    "    for i in other_teams:\n",
    "        rankings.append(i.text.split(\"\\n\")[1])\n",
    "        teams.append(i.text.split(\"\\n\")[4])\n",
    "        matches.append(i.text.split(\"\\n\")[7])\n",
    "        points.append(i.text.split(\"\\n\")[8])\n",
    "        ratings.append(i.text.split(\"\\n\")[9])\n",
    "\n",
    "\n",
    "    for j in range(10):    \n",
    "        print(\"Rank- \",rankings[j])\n",
    "        print(\"Team- \",teams[j])\n",
    "        print(\"Matches- \",matches[j])\n",
    "        print(\"Points- \",points[j])\n",
    "        print(\"Ratings- \",ratings[j],\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "    url='https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting'\n",
    "    r=requests.get(url)\n",
    "    soup=BeautifulSoup(r.content)\n",
    "\n",
    "    ranking=[]\n",
    "    batsmen=[]\n",
    "    team=[]\n",
    "    rating=[]\n",
    "\n",
    "\n",
    "    batsmen1=soup.find('tr',class_='rankings-block__banner')\n",
    "    ranking.append(\"1\")\n",
    "    batsmen.append(batsmen1.text.split(\"\\n\")[20])\n",
    "    team.append(batsmen1.text.split(\"\\n\")[27])\n",
    "    rating.append(batsmen1.text.split(\"\\n\")[31])\n",
    "\n",
    "\n",
    "    other_teams=soup.find_all('tr',class_='table-body')\n",
    "    for i in other_teams:\n",
    "        ranking.append(i.text.split(\"\\n\")[4].split(\" \")[36])\n",
    "        if i.text.split(\"\\n\")[8]==\"(0)\":\n",
    "            batsmen.append(i.text.split(\"\\n\")[13])\n",
    "            team.append(i.text.split(\"\\n\")[17])\n",
    "            rating.append(i.text.split(\"\\n\")[19])\n",
    "\n",
    "        else:\n",
    "            batsmen.append(i.text.split(\"\\n\")[16])\n",
    "            team.append(i.text.split(\"\\n\")[20])\n",
    "            rating.append(i.text.split(\"\\n\")[22])\n",
    "\n",
    "\n",
    "    for j in range(10):    \n",
    "        print(\"Rank- \",ranking[j])\n",
    "        print(\"Batsmen- \",batsmen[j])\n",
    "        print(\"Team- \",team[j])\n",
    "        print(\"Ratings- \",rating[j],\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    url='https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling'\n",
    "    r=requests.get(url)\n",
    "    soup=BeautifulSoup(r.content)\n",
    "\n",
    "    ranking=[]\n",
    "    bowlers=[]\n",
    "    team=[]\n",
    "    rating=[]\n",
    "\n",
    "\n",
    "    bowlers1=soup.find('tr',class_='rankings-block__banner')\n",
    "    ranking.append(\"1\")\n",
    "    bowlers.append(bowlers1.text.split(\"\\n\")[20])\n",
    "    team.append(bowlers1.text.split(\"\\n\")[27])\n",
    "    rating.append(bowlers1.text.split(\"\\n\")[31])\n",
    "\n",
    "\n",
    "    other_teams=soup.find_all('tr',class_='table-body')\n",
    "    for i in other_teams:\n",
    "        ranking.append(i.text.split(\"\\n\")[4].split(\" \")[36])\n",
    "        if i.text.split(\"\\n\")[8]==\"(0)\":\n",
    "            bowlers.append(i.text.split(\"\\n\")[13])\n",
    "            team.append(i.text.split(\"\\n\")[17])\n",
    "            rating.append(i.text.split(\"\\n\")[19])\n",
    "        else:\n",
    "            bowlers.append(i.text.split(\"\\n\")[16])\n",
    "            team.append(i.text.split(\"\\n\")[20])\n",
    "            rating.append(i.text.split(\"\\n\")[22])\n",
    "\n",
    "\n",
    "\n",
    "    for j in range(10):    \n",
    "        print(\"Rank- \",ranking[j])\n",
    "        print(\"Bowler- \",bowlers[j])\n",
    "        print(\"Team- \",team[j])\n",
    "        print(\"Ratings- \",rating[j],\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icc_men(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://www.icc-cricket.com/rankings/womens/team-rankings/odi'\n",
    "def icc_women(url):\n",
    "    r=requests.get(url)\n",
    "    soup=BeautifulSoup(r.content)\n",
    "\n",
    "    rankings=[]\n",
    "    teams=[]\n",
    "    matches=[]\n",
    "    points=[]\n",
    "    ratings=[]\n",
    "\n",
    "    # Scrapping 1st team\n",
    "    team1=soup.find('tr',class_='rankings-block__banner')\n",
    "    rankings.append(team1.text.split(\"\\n\")[1])\n",
    "    teams.append(team1.text.split(\"\\n\")[4])\n",
    "    matches.append(team1.text.split(\"\\n\")[7])\n",
    "    points.append(team1.text.split(\"\\n\")[8])\n",
    "    ratings.append(team1.text.split(\"\\n\")[10].split(\" \")[28])\n",
    "\n",
    "    # Scraping other teams\n",
    "    other_teams=soup.find_all('tr',class_='table-body')\n",
    "    for i in other_teams:\n",
    "        rankings.append(i.text.split(\"\\n\")[1])\n",
    "        teams.append(i.text.split(\"\\n\")[4])\n",
    "        matches.append(i.text.split(\"\\n\")[7])\n",
    "        points.append(i.text.split(\"\\n\")[8])\n",
    "        ratings.append(i.text.split(\"\\n\")[9])\n",
    "\n",
    "    #Printing top 10 teams\n",
    "    for j in range(10):    \n",
    "        print(\"Rank- \",rankings[j])\n",
    "        print(\"Team- \",teams[j])\n",
    "        print(\"Matches- \",matches[j])\n",
    "        print(\"Points- \",points[j])\n",
    "        print(\"Ratings- \",ratings[j],\"\\n\")\n",
    "\n",
    "\n",
    "    # In[3]:\n",
    "\n",
    "\n",
    "    # b) Top 10 women’s ODI Batting players along with the records of their team and rating\n",
    "\n",
    "    url='https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting'\n",
    "    r=requests.get(url)\n",
    "    soup=BeautifulSoup(r.content)\n",
    "\n",
    "    ranking=[]\n",
    "    batting=[]\n",
    "    team=[]\n",
    "    rating=[]\n",
    "\n",
    "    # Scraping rank 1 batting details\n",
    "    bat1=soup.find('tr',class_='rankings-block__banner')\n",
    "    ranking.append(\"1\")\n",
    "    batting.append(bat1.text.split(\"\\n\")[20])\n",
    "    team.append(bat1.text.split(\"\\n\")[27])\n",
    "    rating.append(bat1.text.split(\"\\n\")[31])\n",
    "\n",
    "    # Scraping other batting details\n",
    "    other_teams=soup.find_all('tr',class_='table-body')\n",
    "    for i in other_teams:\n",
    "        ranking.append(i.text.split(\"\\n\")[4].split(\" \")[36])\n",
    "        if i.text.split(\"\\n\")[8]==\"(0)\":\n",
    "            batting.append(i.text.split(\"\\n\")[13])\n",
    "            team.append(i.text.split(\"\\n\")[17])\n",
    "            rating.append(i.text.split(\"\\n\")[19])\n",
    "\n",
    "\n",
    "        else:\n",
    "            batting.append(i.text.split(\"\\n\")[16])\n",
    "            team.append(i.text.split(\"\\n\")[20])\n",
    "            rating.append(i.text.split(\"\\n\")[22])\n",
    "\n",
    "    # Print top 10 Batting women        \n",
    "    for j in range(10):    \n",
    "        print(\"Rank- \",ranking[j])\n",
    "        print(\"Batting- \",batting[j])\n",
    "        print(\"Team- \",team[j])\n",
    "        print(\"Ratings- \",rating[j],\"\\n\")\n",
    "\n",
    "\n",
    "    # In[ ]:\n",
    "\n",
    "\n",
    "    # c) Top 10 women’s ODI all-rounder along with the records of their team and rating.\n",
    "\n",
    "    url='https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder'\n",
    "    r=requests.get(url)\n",
    "    soup=BeautifulSoup(r.content)\n",
    "\n",
    "    ranking=[]\n",
    "    allRounder=[]\n",
    "    team=[]\n",
    "    rating=[]\n",
    "\n",
    "    # Scraping rank 1 all rounder details\n",
    "    AR1=soup.find('tr',class_='rankings-block__banner')\n",
    "    ranking.append(\"1\")\n",
    "    allRounder.append(AR1.text.split(\"\\n\")[20])\n",
    "    team.append(AR1.text.split(\"\\n\")[27])\n",
    "    rating.append(AR1.text.split(\"\\n\")[31])\n",
    "\n",
    "    # Scraping other all rounder details\n",
    "    other_teams=soup.find_all('tr',class_='table-body')\n",
    "    for i in other_teams:\n",
    "        ranking.append(i.text.split(\"\\n\")[4].split(\" \")[36])\n",
    "        if i.text.split(\"\\n\")[8]==\"(0)\":\n",
    "            allRounder.append(i.text.split(\"\\n\")[13])\n",
    "            team.append(i.text.split(\"\\n\")[17])\n",
    "            rating.append(i.text.split(\"\\n\")[19])\n",
    "\n",
    "        else:\n",
    "            allRounder.append(i.text.split(\"\\n\")[16])\n",
    "            team.append(i.text.split(\"\\n\")[20])\n",
    "            rating.append(i.text.split(\"\\n\")[22])\n",
    "\n",
    "    # Printing top 10 all rounders\n",
    "    for j in range(10):    \n",
    "        print(\"Rank- \",ranking[j])\n",
    "        print(\"All Rounder- \",allRounder[j])\n",
    "        print(\"Team- \",team[j])\n",
    "        print(\"Ratings- \",rating[j],\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icc_women(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.cnbc.com/world/?region=world%20:'\n",
    "def cnbc(url):\n",
    "    page= requests.get(url)\n",
    "#     bs = BeautifulSoup(html, \"html.parser\")\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    headline=[]\n",
    "    time=[]\n",
    "    newslink=[]\n",
    "    for i in soup.find_all('a',class_=\"LatestNews-headline\"):\n",
    "        headline.append(i.text)\n",
    "\n",
    "    for i in soup.find_all('span',class_=\"LatestNews-wrapper\"):\n",
    "        time.append(i.text)\n",
    "\n",
    "    for i in soup.find_all(\"a\", class_=\"LatestNews-headline\"):\n",
    "        newslink.append(i.get('href'))\n",
    "\n",
    "    df=pd.DataFrame({'Headline':headline,'Time': time,'URL': newslink})\n",
    "    return df\n",
    "\n",
    "df = cnbc(url)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def journal(url):\n",
    "    page = requests.get(url)\n",
    "\n",
    "    soup= bs(page.content)\n",
    "    soup\n",
    "\n",
    "    papertitle=[]\n",
    "    author=[]\n",
    "    publisheddate=[]\n",
    "    paperurl=[]\n",
    "\n",
    "    for i in soup.find_all('h2', class_=\"sc-1qrq3sd-1 gRGSUS sc-1nmom32-0 sc-1nmom32-1 btcbYu goSKRg\"):\n",
    "        papertitle.append(i.text)\n",
    "\n",
    "    for i in soup.find_all('span', class_=\"sc-1w3fpd7-0 dnCnAO\"):\n",
    "        author.append(i.text)\n",
    "    # author\n",
    "\n",
    "    for i in soup.find_all('span', class_=\"sc-1thf9ly-2 dvggWt\"):\n",
    "        publisheddate.append(i.text.upper())\n",
    "\n",
    "    for i in soup.find_all(\"a\", class_=\"sc-5smygv-0 fIXTHm\"):\n",
    "        paperurl.append(i.get('href'))\n",
    "\n",
    "\n",
    "    df=pd.DataFrame({'Paper Title':papertitle, 'Author':author, 'Published Date':publisheddate,'URL': paperurl})\n",
    "    df.set_index('Published Date', inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles'\n",
    "df = journal(url)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dineout(url):\n",
    "    page=requests.get(url)\n",
    "    soup=bs(page.content)\n",
    "    restaurant_name= []\n",
    "    cuisine = []\n",
    "    location =[]\n",
    "    ratings =[]\n",
    "    imageurl=[]\n",
    "\n",
    "    for i in soup.find_all('a', class_=\"restnt-name ellipsis\"):\n",
    "        restaurant_name.append(i.text)\n",
    "    # name\n",
    "\n",
    "    for i in soup.find_all('span' , class_=\"double-line-ellipsis\"):\n",
    "        cuisine.append(i.text.split('|')[1])\n",
    "    # cuisine\n",
    "\n",
    "    for i in soup.find_all('div', class_=\"restnt-loc ellipsis\"):\n",
    "        location.append(i.text)\n",
    "    # location\n",
    "\n",
    "    for i in soup.find_all('div', class_=\"restnt-rating rating-4\"):\n",
    "        ratings.append(i.text)\n",
    "    # ratings\n",
    "\n",
    "    for i in soup.find_all(\"img\", class_=\"no-img\"):\n",
    "        imageurl.append(i.get('data-src'))\n",
    "\n",
    "\n",
    "    df = pd.DataFrame({'Restaurant':restaurant_name, 'Cuisine':cuisine, 'Location':location, 'Ratings':ratings, 'Images url':imageurl})\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.dineout.co.in/delhi-restaurants/buffet-special'\n",
    "dine = dineout(url)\n",
    "dine.set_index('Restaurant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def googleScholar(url):\n",
    "    page = requests.get(url)\n",
    "    content=bs(page.content)\n",
    "#     print(content)\n",
    "    rank=[]\n",
    "    publication=[]\n",
    "    h5_index=[]\n",
    "    h5_median=[]\n",
    "\n",
    "    for i in content.find_all('td', class_=\"gsc_mvt_p\"):\n",
    "        rank.append(i.text)  \n",
    "\n",
    "    for i in content.find_all('td', class_=\"gsc_mvt_t\"):\n",
    "        publication.append(i.text)\n",
    "\n",
    "    for i in content.find_all('a', class_=\"gs_ibl gsc_mp_anchor\"):\n",
    "        h5_index.append(i.text)\n",
    "\n",
    "    for i in content.find_all('span', class_=\"gs_ibl gsc_mp_anchor\"):\n",
    "        h5_median.append(i.text)\n",
    "\n",
    "    df= pd.DataFrame({'Ranking':rank, 'Publication':publication, 'h5-index': h5_index[:100], 'h5-median':h5_median})\n",
    "    df.set_index('Ranking', inplace=True)\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url = 'https://scholar.google.com/citations?view_op=top_venues&hl=en'\n",
    "googleScholar(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
